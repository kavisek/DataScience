{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Forecasting\n",
    "\n",
    "<span>This notebook demonstrates my solution to a time series problem posted on Kaggle. In the dataset, we have near 1 million training records from 1000 stores. We will need to use this information to predict the total Sales for each Day in the test set, which is about 40,000 records. During the modeling process, I created a few benchmark models to compare my LSTM Neural Network's performance against near the very end.</span>\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Rossman Dataset: https://www.kaggle.com/c/rossmann-store-sales/overview\n",
    "\n",
    "### Import Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:58:56.162923Z",
     "start_time": "2018-09-24T08:58:51.755141Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4437170818c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Import modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import generic data science packages\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# Import modules\n",
    "import datetime\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib as mpl\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from keras import models, layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Set pandas options\n",
    "pd.set_option('max_columns',1000)\n",
    "pd.set_option('max_rows',30)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Set plotting options\n",
    "mpl.rcParams['figure.figsize'] = (8.0, 7.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data\n",
    "\n",
    "The training and testing set have been given to us into separate CSV files, with an external file for the additional store data. I found empty string values in the data after importing the store table, so I wrote a quick \"for\" loop in the cell below, changing these empty string to null values. We will address how to handle the null values later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:01.978386Z",
     "start_time": "2018-09-24T08:58:56.175268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set your local database parameters \n",
    "db_username = 'root'\n",
    "db_password = 'mypassword'\n",
    "host = '127.0.0.1'\n",
    "port = '3306'\n",
    "db_name = 'rossman'\n",
    "\n",
    "# Create a MySQLEngine\n",
    "engine = sqlalchemy.create_engine('mysql+mysqldb://'+db_username+':'+db_password+'@\\\n",
    "'+host+':'+port+'/'+db_name)\n",
    "\n",
    "# Connect to database\n",
    "engine.connect();\n",
    "\n",
    "# Import data from SQL\n",
    "train_df = pd.read_sql(\"\"\" SELECT * FROM train\"\"\", engine,\n",
    "                      parse_dates=['Date'],\n",
    "                      ).dropna(how='any', axis=0)\n",
    "test_df = pd.read_sql(\"\"\" SELECT * FROM test\"\"\", engine,\n",
    "                     index_col='Id', parse_dates=['Date']\n",
    "                     ).dropna(how='any', axis=0)\n",
    "stores = pd.read_sql(\"\"\" SELECT * FROM store\"\"\", engine,\n",
    "                     ).dropna(how='any', axis=0)\n",
    "\n",
    "# Rename salse column to target\n",
    "train_df.rename(columns={'Sales':'target'}, inplace=True)\n",
    "\n",
    "# Replace empty string in store dataframe with Null Values\n",
    "for col in stores:\n",
    "    stores[col] = stores[col].replace('',np.nan)\n",
    "\n",
    "# Down Sample Dataset\n",
    "#train_df = train_df.sample(10_000)\n",
    "#test_df = test_df.sample(10_000)\n",
    "    \n",
    "# View Store Dataframe\n",
    "stores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:02.047733Z",
     "start_time": "2018-09-24T08:59:02.038166Z"
    }
   },
   "outputs": [],
   "source": [
    "# View head of the dataframe\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:02.114094Z",
     "start_time": "2018-09-24T08:59:02.102662Z"
    }
   },
   "outputs": [],
   "source": [
    "# View head of the dataframe\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Completness\n",
    "\n",
    "\"MissingNo\" is a library that makes it easy to plot the completeness of your dataset. The training and testing data frames do not have any null values, but the store dataframe is missing a few data points in a couple of features. Remember these were the empty string values that we changed to null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:05.046138Z",
     "start_time": "2018-09-24T08:59:02.170207Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the null values from train, tets, and store dataframes\n",
    "msno.matrix(train_df,  figsize=(8,6), fontsize=10);\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Observations');\n",
    "\n",
    "msno.matrix(test_df,  figsize=(8,6), fontsize=10);\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Observations');\n",
    "\n",
    "msno.matrix(stores,  figsize=(8,6), fontsize=10);\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:05.440435Z",
     "start_time": "2018-09-24T08:59:05.104276Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the distribution of target classes\n",
    "train_df.target.plot.hist(figsize=(9,5),color='#BAE4B3', edgecolor='black')\n",
    "plt.axvline(train_df.target.mean(), color='r', linestyle='dashed', \n",
    "            linewidth=2);\n",
    "plt.title('Sales Histogram')\n",
    "plt.ylabel('Salse')\n",
    "plt.xlabel('Number of Records');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:05.716611Z",
     "start_time": "2018-09-24T08:59:05.497213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a barplot of the target clasees in our training set\n",
    "# (1 = posinous, 0 = eatable by humans)\n",
    "train_df.DayOfWeek.value_counts().sort_index().plot.barh(\n",
    "                figsize = (9,5),\n",
    "                grid=False, \n",
    "                color=['#edf8e9','#c7e9c0','#a1d99b','#74c476',\n",
    "                  '#41ab5d','#238b45','#005a32'], \n",
    "                width=0.5, edgecolor='black',\n",
    "                hatch='|')\n",
    "plt.title('Target Outcomes')\n",
    "plt.ylabel('Target Class')\n",
    "plt.xlabel('Number of Records');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:06.283814Z",
     "start_time": "2018-09-24T08:59:05.785076Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot counts for days in the dataset a store is open\n",
    "plt.figure()\n",
    "open_counts = train_df.Open.value_counts()\n",
    "open_counts.plot.barh(figsize=(9,3),grid=False, \n",
    "                      color=['#CFCCC7','#3F3F3F'],\n",
    "                      width=0.25,edgecolor='w')\n",
    "plt.title('Open Counts')\n",
    "plt.ylabel('Open Classes')\n",
    "plt.xlabel('Number of Records');\n",
    "\n",
    "# Plot number of time that we are open during state holidary (probably never)\n",
    "plt.figure()\n",
    "sthol_counts = train_df.StateHoliday.value_counts()\n",
    "sthol_counts = pd.Series([sthol_counts.values[0], 0 ]) # plotting fix\n",
    "sthol_counts.plot.barh(figsize=(9,3),grid=False, \n",
    "                       color=['#CFCCC7','#3F3F3F'], \n",
    "                       width=0.25,edgecolor='w')\n",
    "plt.title('Stat Holiday Counts')\n",
    "plt.ylabel('Stat Holiday Classes')\n",
    "plt.xlabel('Number of Records');\n",
    "\n",
    "# Plot the number of days in the datset that are affected by school holidays\n",
    "plt.figure()\n",
    "schol_counts = train_df.SchoolHoliday.value_counts()\n",
    "schol_counts.plot.barh(figsize=(9,3),grid=False, \n",
    "                       color=['#CFCCC7','#3F3F3F'], \n",
    "                       width=0.25,edgecolor='w')\n",
    "plt.title('School Holiday Counts')\n",
    "plt.ylabel('Target Class')\n",
    "plt.xlabel('Number of Records');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:06.592661Z",
     "start_time": "2018-09-24T08:59:06.345515Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot sales by day of the week\n",
    "sales_by_week = train_df.groupby('DayOfWeek')['target'].mean()\n",
    "sales_by_week.plot.barh(\n",
    "                figsize = (9,5),\n",
    "                grid=False, \n",
    "                color=['#edf8e9','#c7e9c0','#a1d99b','#74c476',\n",
    "                  '#41ab5d','#238b45','#005a32'], \n",
    "                width=0.5, edgecolor='black',\n",
    "                hatch='o')\n",
    "plt.title('Average Salse Per Day of Week')\n",
    "plt.ylabel('Day of Week')\n",
    "plt.xlabel('Sales');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:07.268599Z",
     "start_time": "2018-09-24T08:59:06.659124Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot a line plot showing growth by year\n",
    "annual_growth = train_df.set_index('Date')\n",
    "annual_growth = annual_growth.resample('1M').sum()\n",
    "annual_growth.plot.area(y='target', figsize=(15,5), \n",
    "                        color='#A1D99B', alpha=0.5)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.title('Sales over Time')\n",
    "plt.xlabel('Datetime')\n",
    "plt.ylabel('Sales');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Incosistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:07.341764Z",
     "start_time": "2018-09-24T08:59:07.337885Z"
    }
   },
   "outputs": [],
   "source": [
    "# Print the featuer names from each dataframe\n",
    "print('Training Dataset:', train_df.columns, '\\n')\n",
    "print('Testing Dataset:', test_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that training dataset has a column that is not present in our Testing Set. Therefore let's map the customers count to the storse dataset than merge the total customer counts back into the main testing and training dataframes in the next cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:07.478831Z",
     "start_time": "2018-09-24T08:59:07.413036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store customer information seperately\n",
    "customers_df = train_df[['Store','Customers']]\n",
    "customers_df = customers_df.groupby('Store')['Customers'].sum().to_frame()\n",
    "customers_df.rename(columns={'Customers':'Lifetime Customers'}, inplace=True)\n",
    "train_df = train_df.drop('Customers', axis=1)\n",
    "customers_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:08.755652Z",
     "start_time": "2018-09-24T08:59:07.554950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Copy dataframe for cleaning and feature engineering\n",
    "train_fdf = train_df.copy()\n",
    "test_fdf = test_df.copy()\n",
    "store_fdf = stores.copy()\n",
    "\n",
    "train_fdf = train_fdf.merge(stores, how='left', on='Store')\n",
    "test_fdf = test_fdf.merge(stores, how='left', on='Store')\n",
    "\n",
    "# Deal will null values from merge\n",
    "train_fdf.CompetitionOpenSinceMonth.fillna(-1, inplace=True)\n",
    "train_fdf.CompetitionOpenSinceYear.fillna(datetime.datetime.now().year + 1, inplace=True)\n",
    "train_fdf.Promo2.fillna(-1, inplace=True)\n",
    "train_fdf.Promo2SinceWeek.fillna(-1, inplace=True)\n",
    "train_fdf.Promo2SinceYear.fillna(datetime.datetime.now().year + 1, inplace=True)\n",
    "train_fdf.PromoInterval.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Deal will null values after merge\n",
    "train_fdf.Assortment.fillna('Unknown', inplace=True)\n",
    "train_fdf.StoreType.fillna('Uknown', inplace=True)\n",
    "train_fdf.CompetitionDistance.fillna(train_fdf.CompetitionDistance.mean(), inplace =True)\n",
    "\n",
    "# View existing null value of the train dataframe\n",
    "train_fdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:08.947834Z",
     "start_time": "2018-09-24T08:59:08.895346Z"
    }
   },
   "outputs": [],
   "source": [
    "# Deal will null values from merge\n",
    "test_fdf.CompetitionOpenSinceMonth.fillna(-1, inplace=True)\n",
    "test_fdf.CompetitionOpenSinceYear.fillna(datetime.datetime.now().year + 1, inplace=True)\n",
    "test_fdf.Promo2.fillna(-1, inplace=True)\n",
    "test_fdf.Promo2SinceWeek.fillna(-1, inplace=True)\n",
    "test_fdf.Promo2SinceYear.fillna(datetime.datetime.now().year + 1, inplace=True)\n",
    "test_fdf.PromoInterval.fillna('Unknown', inplace=True)\n",
    "\n",
    "# Deal will null values after merge\n",
    "test_fdf.Assortment.fillna('Unknown', inplace=True)\n",
    "test_fdf.StoreType.fillna('Uknown', inplace=True)\n",
    "test_fdf.CompetitionDistance.fillna(train_fdf.CompetitionDistance.mean(), inplace =True)\n",
    "\n",
    "# View existing null value of the test dataframe\n",
    "test_fdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:09.678196Z",
     "start_time": "2018-09-24T08:59:09.094735Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-268fe8ab4340>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Plot feature correlation matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_fdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGreens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Feature Correlation Heatmap (Before Feature Engineering)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot feature correlation matrix\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(train_fdf.corr(), cmap=plt.cm.Greens, linewidth=1)\n",
    "plt.title('Feature Correlation Heatmap (Before Feature Engineering)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Features');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strange correlation between StateHoliday and every other feature in the training set is because the StateHoliday feature is an entire feature with just the value \"0\". Therefore we will remove this feature at the tail end of our feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:25.700204Z",
     "start_time": "2018-09-24T08:59:10.290224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append Customer Information\n",
    "train_fdf = train_fdf.merge(customers_df, how='left', on='Store')\n",
    "test_fdf = test_fdf.merge(customers_df, how='left', on='Store')\n",
    "\n",
    "\n",
    "# One Hot Encoding Train Non-Ordinal Feature\n",
    "train_fdf = pd.get_dummies(train_fdf, drop_first=True,\n",
    "               columns=['Assortment','Store','StoreType','Promo2',\n",
    "                        'Promo2SinceYear','PromoInterval'])\n",
    "\n",
    "# One Hot Encoding Test Non-Ordinal Feature\n",
    "test_fdf = pd.get_dummies(test_fdf, drop_first=True,\n",
    "               columns=['Assortment','Store','StoreType','Promo2',\n",
    "                        'Promo2SinceYear','PromoInterval'])\n",
    "\n",
    "# Expand Train DateTime Feature\n",
    "train_fdf['Year'] = train_fdf['Date'].dt.year\n",
    "train_fdf['Day'] = train_fdf['Date'].dt.day\n",
    "train_fdf['Montah']= train_fdf['Date'].dt.month\n",
    "train_fdf.drop('Date', axis=1 ,inplace=True)\n",
    "\n",
    "# Expand Test DateTime Feature\n",
    "test_fdf['Year'] = test_fdf['Date'].dt.year\n",
    "test_fdf['Day'] = test_fdf['Date'].dt.day\n",
    "test_fdf['Month']= test_fdf['Date'].dt.month\n",
    "test_fdf.drop('Date', axis=1 ,inplace=True)\n",
    "\n",
    "# Change the Opent Store Year into a Year Ago\n",
    "train_fdf.CompetitionOpenSinceYear = (train_fdf.CompetitionOpenSinceYear.astype(int) \n",
    "                                      - datetime.datetime.now().year)\n",
    "test_fdf.CompetitionOpenSinceYear = (test_fdf.CompetitionOpenSinceYear.astype(int) \n",
    "                                      - datetime.datetime.now().year)\n",
    "# Drop StateHoliday Feature\n",
    "train_df.drop('StateHoliday', axis=1, inplace=True )\n",
    "test_df.drop('StateHoliday', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding + Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T08:59:33.619583Z",
     "start_time": "2018-09-24T08:59:31.315767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape: (986159, 1146)\n",
      "Training Target Shape: (986159,)\n"
     ]
    }
   ],
   "source": [
    "# Clean us some remaining object dtypes into ints\n",
    "for df in [train_fdf, test_fdf]:\n",
    "    for col in df.select_dtypes(include='object'):\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "# Target Values\n",
    "y = train_fdf.target\n",
    "X = train_fdf.drop('target', axis=1)\n",
    "print(f'Training Data Shape:',X.shape)\n",
    "print(f'Training Target Shape:',y.shape)\n",
    "\n",
    "# Create KFold cross validation rule without shuffling since we are \n",
    "# dealing with a time series\n",
    "kfold = KFold(n_splits=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T05:40:27.670097Z",
     "start_time": "2018-09-24T05:20:12.610575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross Validation Score - Mean Absolute Erorr: -2108.44595\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Pipeline \n",
    "lr_pipeline = make_pipeline(PCA(n_components=10),StandardScaler(), LinearRegression())\n",
    "lr_scores = cross_val_score(lr_pipeline, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "print(f'Linear Regression Cross Validation Score - Mean Absolute Erorr: {lr_scores.mean():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T06:26:20.412199Z",
     "start_time": "2018-09-24T05:43:07.338603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Cross Validation Score - Mean Absolute Erorr: -779.27056\n"
     ]
    }
   ],
   "source": [
    "# Linear Regression Pipeline \n",
    "rfr_pipeline = make_pipeline(PCA(n_components=10),StandardScaler(), RandomForestRegressor())\n",
    "rfr_scores = cross_val_score(rfr_pipeline, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "print(f'Linear Regression Cross Validation Score - Mean Absolute Erorr: {rfr_scores.mean():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T18:40:44.886452Z",
     "start_time": "2018-09-24T09:23:52.455089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Densly Connected Cross Validation Score -  Mean Absolute Error : -5905.11181\n"
     ]
    }
   ],
   "source": [
    "# Dense Neural Network\n",
    "def create_ds_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(128, activation='relu', input_shape=(10,)))\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer='rmsprop', loss='mean_absolute_error', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "ds_model = KerasClassifier(build_fn=create_ds_model, epochs=50, batch_size=10, verbose=0)\n",
    "\n",
    "# Linear Regression Pipeline \n",
    "ds_pipeline = make_pipeline(PCA(n_components=10),StandardScaler(), ds_model)\n",
    "ds_scores = cross_val_score(ds_pipeline, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "print(f'Densly Connected Cross Validation Score -  Mean Absolute Error : {ds_scores.mean():.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-09-24T19:38:58.388Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.LSTM(units=128))\n",
    "    model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='mean_absolute_error', # Cross-entropy\n",
    "                optimizer='Adam', # Adam optimization\n",
    "                metrics=['mse']) # Accuracy performance metric\n",
    "    return model\n",
    "\n",
    "lstm_model = KerasClassifier(build_fn=create_lstm_model, epochs=50, batch_size=10, verbose=1)\n",
    "lstm_pipeline = make_pipeline(PCA(n_components=10),StandardScaler(), lstm_model)\n",
    "lstm_scores = cross_val_score(lstm_pipeline, X, y, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "print(f'Densly Connected Cross Validation Score -  Mean Absolute Error : {lstm_scores.mean():.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-24T19:37:15.999627Z",
     "start_time": "2018-09-24T19:37:15.986799Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'KerasClassifier' object has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-a41d9a69e342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'KerasClassifier' object has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model\n",
    "\n",
    "I will be using the last LSTM model as my final model for this problem. Generally at this stage, we would begin tuning the hyperparameters via Grid Search, but for the sake of not destroying my local computer, we will leave it here for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and scale the entire dataset\n",
    "decomposer = PCA(n_components=10)\n",
    "X_decomposed = decomposer.fit_transform(X)\n",
    "scaler = StandardScaler()\n",
    "X_dec_scal = scaler.fit_transform(X_decomposed)\n",
    "\n",
    "# Build LSTM Model again but on the entire dataset\n",
    "model = models.Sequential()\n",
    "model.add(layers.Embedding(input_dim=train_x.shape[1], output_dim=128))\n",
    "model.add(layers.LSTM(units=128))\n",
    "model.add(layers.Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='neg_mean_absolute_error', # Cross-entropy\n",
    "            optimizer='Adam', # Adam optimization\n",
    "            metrics=['mse']) # Accuracy performance metric\n",
    "model.fit(X_des_scal, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Kavi Sekhon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
